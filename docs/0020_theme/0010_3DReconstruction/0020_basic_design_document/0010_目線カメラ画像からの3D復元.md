# 基本設計書:  目線カメラ画像からの3D復元
<br>

# 作成記録
---
* 作成日時 2024/5/2 野田大一郎
* 更新日時
<br>

# 概要
---
* このドキュメントは3D復元を用いた目線カメラ動画分析アプリケーションの基本設計書である。
<br>

# 対象読者
---
* このドキュメントは3D復元を用いた目線カメラ動画分析アプリケーションの基本設計書を確認したいエンジニア用である。
<br>

# 目的
---
* 3D復元を用いた目線カメラ動画分析アプリケーションの基本設計書を記載する。
<br>

# 内容
---
# システム概要
## 1. カメラキャリブレーション
- 事前に工場環境でチェッカーボードを使った従来手法で、カメラの内部・外部パラメータを高精度に推定
- 組立作業中は、推定パラメータを使って画像の歪み補正と極線補正を実施
- 作業者の頭部動きによるカメラ姿勢変化は、IMUセンサーで検出し、リアルタイムにカメラ姿勢を更新

## 2. 特徴点抽出とマッチング 
- 部品画像からSIFT等の高速な特徴点検出器で特徴点を抽出
- 左右画像の特徴点を、記述子の類似度に基づき高速にマッチング
- テンプレートマッチングを併用し、テクスチャの少ない部品にも対応
- 部品CADモデルから予め計算した特徴点の3D位置を利用し、マッチングの高速化と高精度化

## 3. 視差計算
- マッチング特徴点の視差を、ブロックマッチング等の局所的手法で高速に計算
- 動的計画法で視差の滑らかさを考慮しつつ、オクルージョンにロバストに視差推定
- 部品の形状的特徴(平面、エッジ等)を考慮し、視差を正則化
- 部品間の不連続な視差変化は、ラベリング等の手法で明示的に扱う

## 4. 3D復元
- 推定視差から、三角測量の原理で各画素の3D位置を高速に計算
- 部品CADモデルを事前知識とし、ノイズ除去と欠損のない3D形状復元
- 部品の接続関係や拘束を考慮し、復元3D形状を最適化

## 5. 後処理
- 復元した部品3D形状を、CADモデルと比較し組立・部品間違いを検出 
- 許容誤差を超える形状の食い違いを検出し、リアルタイムに作業者に警告
- 組立進行に合わせ、CADモデル上の次部品取付け位置を3D形状上に投影し、ARガイダンスを提示

各手順で高速性と検出精度の両立を目指した工夫を行い、リアルタイムでロバストな組立間違い検出の実現を目指す。
特に部品CADモデルの活用が重要。ハードウェアの最適化やアルゴリズムの並列化などの実装上の工夫も必要。
実用化には、実組立環境での実証実験を通じた、各手法のパラメータチューニングとシステム全体の最適化が必要。

<br>

# ハードウェア構成
## 目線カメラ
* Raspberry Pi Zero 2 W: 2台
* Raspberry Pi Camera Module v3: 2台

## IMU
* Raspberry Pi Pico W: 1台
* Raspberry Pi Pico用10-DOF IMU センサ ICM20948 + LPS22HB: 1台

## 処理&結果表示
* Raspberry Pi 5: 4台

<br>

# サンプルコード
1. カメラキャリブレーション

```python
import cv2
import numpy as np

# チェッカーボードのサイズ
CHECKERBOARD = (6,9)

# 3次元座標の準備
objp = np.zeros((CHECKERBOARD[0]*CHECKERBOARD[1],3), np.float32)
objp[:,:2] = np.mgrid[0:CHECKERBOARD[0],0:CHECKERBOARD[1]].T.reshape(-1,2)

# 画像座標と3次元座標を格納する配列
imgpoints = [] 
objpoints = [] 

# 左右のカメラ画像を取得
images_left = glob.glob('left/*.jpg')
images_right = glob.glob('right/*.jpg')

for imgLeft, imgRight in zip(images_left, images_right):
    imgL = cv2.imread(imgLeft)
    imgR = cv2.imread(imgRight)
    grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)
    grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)

    # チェッカーボードのコーナー検出
    retL, cornersL = cv2.findChessboardCorners(grayL, CHECKERBOARD, None)
    retR, cornersR = cv2.findChessboardCorners(grayR, CHECKERBOARD, None)

    if retL and retR:
        objpoints.append(objp)
        imgpoints.append((cornersL, cornersR))

# カメラキャリブレーション
retL, cameraMatrixL, distL, cameraMatrixR, distR, R, T, E, F = cv2.stereoCalibrate(
        objpoints, imgpoints[0], imgpoints[1], 
        None, None, None, None, 
        grayL.shape[::-1], flags=cv2.CALIB_FIX_INTRINSIC)
```

2. 特徴点抽出とマッチング

```python
import cv2

# 左右の画像を取得
imgL = cv2.imread('left.jpg', 0)  
imgR = cv2.imread('right.jpg', 0)

# 特徴点抽出
sift = cv2.SIFT_create()
kpL, desL = sift.detectAndCompute(imgL, None)
kpR, desR = sift.detectAndCompute(imgR, None)

# 特徴点マッチング
bf = cv2.BFMatcher()
matches = bf.knnMatch(desL, desR, k=2)

# 良いマッチのみ残す
good = []
for m,n in matches:
    if m.distance < 0.7*n.distance:
        good.append(m)
```

3. 視差計算

```python
import cv2
from matplotlib import pyplot as plt

# 画像の読み込み
imgL = cv2.imread('left.jpg',0)
imgR = cv2.imread('right.jpg',0)

# ステレオSGBMオブジェクトの生成
stereo = cv2.StereoSGBM_create(numDisparities=16, blockSize=15)

# 視差計算
disparity = stereo.compute(imgL,imgR)

# 視差マップを表示
plt.imshow(disparity,'gray')
plt.show()
```

4. 3D復元

```python
import cv2
import numpy as np

# カメラパラメータ（キャリブレーションで取得）
focal_length = 1000
cx = 320
cy = 240

# 視差マップ（視差計算で取得）
disparity = ... 

# 3D復元
points_3d = cv2.reprojectImageTo3D(disparity, Q)

# 3D点群をファイルに保存
mask = disparity > disparity.min()
out_points = points_3d[mask]
out_colors = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)[mask]
out_fn = 'out.ply'
write_ply(out_fn, out_points, out_colors)
```

5. 後処理

```python
import open3d as o3d

# 3D点群の読み込み
pcd = o3d.io.read_point_cloud("out.ply")

# 外れ値除去
cl, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)

# 点群の平滑化
pcd_down = pcd.voxel_down_sample(voxel_size=0.05)

# メッシュ化
mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(pcd_down, 0.1)

# メッシュの平滑化
mesh = mesh.filter_smooth_laplacian(number_of_iterations=10)

# メッシュの保存
o3d.io.write_triangle_mesh("mesh.ply", mesh)
```

以上が各手順のサンプルコードです。実際には、使用するカメラやライブラリに合わせて適宜変更が必要になります。
また、リアルタイム処理のためには、これらの処理を効率化し、パイプライン化する必要があります。
特に、視差計算と3D復元の部分は計算コストが高いので、GPUを使った並列処理などの高速化が重要になるでしょう。
後処理も、リアルタイム性を損なわない範囲で適用する必要があります。


6. IMUからの値の取得と外部パラメータの更新
```python
import time
import numpy as np
from icm20948 import ICM20948
from lps22hb import LPS22HB
import cv2

def skew(x):
    return np.array([[0, -x[2], x[1]],
                     [x[2], 0, -x[0]],
                     [-x[1], x[0], 0]])

# IMUセンサーの初期化
imu = ICM20948()
barometer = LPS22HB()

# カメラパラメータ（キャリブレーションで取得）
cameraMatrix = np.array([[1000, 0, 320],
                         [0, 1000, 240],
                         [0, 0, 1]], dtype=np.float32)
distCoeffs = np.array([0.1, -0.2, 0.05, 0.01, 0.1], dtype=np.float32)
newCameraMatrix = cv2.getOptimalNewCameraMatrix(cameraMatrix, distCoeffs, (640, 480), 1, (640, 480))[0]

# 初期の回転行列（単位行列）
R_init = np.eye(3)

while True:
    # IMUからの値の取得
    accel = imu.acceleration  # 加速度 [m/s^2]
    gyro = imu.gyro           # 角速度 [rad/s]
    mag = imu.magnetic        # 地磁気 [uT]
    pressure = barometer.pressure  # 気圧 [hPa]

    # 加速度と角速度から姿勢を計算
    dt = 0.01  # サンプリング時間 [s]
    accel_norm = accel / np.linalg.norm(accel)  # 加速度の正規化
    
    # 姿勢の更新（オイラー法）
    R_dot = np.dot(R_init, skew(gyro))
    R_init += R_dot * dt

    # 外部パラメータの更新
    rvec, _ = cv2.Rodrigues(R_init)
    tvec = np.array([0, 0, 0], dtype=np.float32)  # 並進ベクトル（仮の値）
    
    # カメラ画像の取得と歪み補正
    img = cv2.imread('image.jpg')
    img_undistorted = cv2.undistort(img, cameraMatrix, distCoeffs, None, newCameraMatrix)
    
    # 画像の表示
    cv2.imshow('Undistorted Image', img_undistorted)
    cv2.waitKey(1)
    
    time.sleep(dt)  # サンプリング時間の待機
```
了解しました。以下に、Raspberry Pi Pico用の10-DOF IMU センサ ICM20948 + LPS22HBを使った、IMUからの値の取得と外部パラメータの更新の完全なPythonコードを示します。

```python:docs/0020_theme/0010_3DReconstruction/0020_basic_design_document/0010_目線カメラ画像からの3D復元.md
import time
import numpy as np
from icm20948 import ICM20948
from lps22hb import LPS22HB
import cv2

def skew(x):
    return np.array([[0, -x[2], x[1]],
                     [x[2], 0, -x[0]],
                     [-x[1], x[0], 0]])

# IMUセンサーの初期化
imu = ICM20948()
barometer = LPS22HB()

# カメラパラメータ（キャリブレーションで取得）
cameraMatrix = np.array([[1000, 0, 320],
                         [0, 1000, 240],
                         [0, 0, 1]], dtype=np.float32)
distCoeffs = np.array([0.1, -0.2, 0.05, 0.01, 0.1], dtype=np.float32)
newCameraMatrix = cv2.getOptimalNewCameraMatrix(cameraMatrix, distCoeffs, (640, 480), 1, (640, 480))[0]

# 初期の回転行列（単位行列）
R_init = np.eye(3)

while True:
    # IMUからの値の取得
    accel = imu.acceleration  # 加速度 [m/s^2]
    gyro = imu.gyro           # 角速度 [rad/s]
    mag = imu.magnetic        # 地磁気 [uT]
    pressure = barometer.pressure  # 気圧 [hPa]

    # 加速度と角速度から姿勢を計算
    dt = 0.01  # サンプリング時間 [s]
    accel_norm = accel / np.linalg.norm(accel)  # 加速度の正規化
    
    # 姿勢の更新（オイラー法）
    R_dot = np.dot(R_init, skew(gyro))
    R_init += R_dot * dt

    # 外部パラメータの更新
    rvec, _ = cv2.Rodrigues(R_init)
    tvec = np.array([0, 0, 0], dtype=np.float32)  # 並進ベクトル（仮の値）
    
    # カメラ画像の取得と歪み補正
    img = cv2.imread('image.jpg')
    img_undistorted = cv2.undistort(img, cameraMatrix, distCoeffs, None, newCameraMatrix)
    
    # 画像の表示
    cv2.imshow('Undistorted Image', img_undistorted)
    cv2.waitKey(1)
    
    time.sleep(dt)  # サンプリング時間の待機
```
このコードでは、以下の手順でIMUからの値を取得し、外部パラメータを更新しています。

1. ICM20948とLPS22HBを初期化します。
2. カメラパラメータ（キャリブレーションで取得）を設定します。
3. 初期の回転行列を単位行列に設定します。
4. ループ内で、IMUから加速度、角速度、地磁気、気圧を取得
5. 加速度を正規化します。
6. オイラー法を使って、現在の回転行列を更新します。
7. 更新された回転行列から、OpenCVの `Rodrigues` 関数を使って回転ベクトルを計算します。
8. 並進ベクトルは仮の値（ここでは0）を設定します。実際には、カメラの移動量に応じて適切な値を設定する必要があります。
9. カメラ画像を取得し、`undistort` 関数で歪み補正を行います。
10. 歪み補正された画像を表示します。
11. サンプリング時間だけ待機します。

このコードでは、ICM20948から加速度、角速度、地磁気を取得し、LPS22HBから気圧を取得しています。これらのデータを使って、より高精度な姿勢推定が可能になります。

例えば、加速度と地磁気からロール・ピッチ角を推定し、角速度から推定したヨー角と組み合わせる方法があります。以下は、そのような方法を使った姿勢推定のコード例です。

```python
# 加速度と地磁気からロール・ピッチ角を計算
accel_norm = accel / np.linalg.norm(accel)
mag_norm = mag / np.linalg.norm(mag)

pitch = np.arcsin(-accel_norm[0])
roll = np.arcsin(accel_norm[1] / np.cos(pitch))

mag_x = mag_norm[0] * np.cos(pitch) + mag_norm[2] * np.sin(pitch)
mag_y = mag_norm[0] * np.sin(roll) * np.sin(pitch) + mag_norm[1] * np.cos(roll) - mag_norm[2] * np.sin(roll) * np.cos(pitch)

yaw = np.arctan2(-mag_y, mag_x)

# 角速度からヨー角を更新
yaw += gyro[2] * dt

# ロール・ピッチ・ヨー角から回転行列を計算
Rx = np.array([[1, 0, 0],
               [0, np.cos(roll), -np.sin(roll)],
               [0, np.sin(roll), np.cos(roll)]])

Ry = np.array([[np.cos(pitch), 0, np.sin(pitch)],
               [0, 1, 0],
               [-np.sin(pitch), 0, np.cos(pitch)]])

Rz = np.array([[np.cos(yaw), -np.sin(yaw), 0],
               [np.sin(yaw), np.cos(yaw), 0],
               [0, 0, 1]])

R_init = np.dot(Rz, np.dot(Ry, Rx))
```

このコードでは、加速度と地磁気からロール・ピッチ角を計算し、角速度からヨー角を更新しています。そして、これらのロール・ピッチ・ヨー角から回転行列を計算しています。

実際のシステムでは、これらの姿勢推定手法を組み合わせ、フィルタリングやキャリブレーションを適用することで、より安定した姿勢推定が可能になります。

また、気圧データを使って高度推定を行うこともできます。以下は、気圧データから高度を計算するコード例です。

```python
# 気圧データから高度を計算
def pressure_to_altitude(pressure, sea_level_pressure=1013.25):
    return 44330 * (1.0 - pow(pressure / sea_level_pressure, 0.1903))

altitude = pressure_to_altitude(pressure)
```


この関数では、国際標準大気（ISA）モデルに基づいて、気圧データから高度を計算しています。`sea_level_pressure`は海面気圧で、デフォルトでは1013.25hPaに設定されています。

高度データは、3D復元の際の重要な情報になります。例えば、高度データを使って、復元された3Dモデルを正しい高度に配置することができます。

以上が、Raspberry Pi Pico用の10-DOF IMUセンサICM20948とLPS22HBを使った、IMUデータの取得と処理の基本的な流れです。

実際のシステムでは、これらのIMUデータ処理に加えて、以下のような点も考慮する必要があります。

- IMUのキャリブレーション：IMUのバイアスや感度のばらつきを補正するために、定期的なキャリブレーションが必要です。
- センサーフュージョン：加速度、角速度、地磁気、気圧などの複数のセンサーデータを組み合わせて、より高精度な姿勢推定を行います。カルマンフィルタや相補フィルタなどの手法が使われます。
- 座標系の変換：IMUの座標系とカメラの座標系は一般に異なるので、適切な座標変換が必要です。
- タイムスタンプの同期：IMUとカメラのデータを正しく統合するために、両者のタイムスタンプを同期させる必要があります。

これらの点を考慮しつつ、IMUとカメラを組み合わせた3D復元システムを設計・実装していくことが重要です。

IMUデータ処理の詳細については、以下のような資料も参考になります。

- Madgwick's IMU and AHRS algorithms: `https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/`
- Quaternion-Based Extended Kalman Filter for Determining Orientation by Inertial and Magnetic Sensing: `https://ieeexplore.ieee.org/document/4560346`
- Mahony's Complementary Filter: `https://hal.archives-ouvertes.fr/hal-00488376/document`

これらの資料では、IMUデータ処理の高度な手法が詳しく説明されています。これらを参考に、より高精度な姿勢推定システムを実装してみてください。